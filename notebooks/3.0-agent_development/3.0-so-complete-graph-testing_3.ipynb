{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c57516d8-1607-4573-bcdd-c78af6861c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 03:35:23,103 - currensee.core.secrets - INFO - PROJECT_ID not found, attempting to load from .env file\n",
      "2025-05-20 03:35:23,106 - currensee.core.secrets - INFO - Loaded environment from /home/jupyter/so_currensee/tools_test/currensee/.env\n",
      "2025-05-20 03:35:23,108 - currensee.core.secrets - INFO - SecretManager initialized with project_id: adsp-34002-on02-sopho-scribe\n"
     ]
    }
   ],
   "source": [
    "from currensee.agents.complete_graph import compiled_graph\n",
    "from currensee.agents.tools.finance_tools import generate_macro_table\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0382d110-7666-4cf9-8664-a6ec2799f35e",
   "metadata": {},
   "source": [
    "## Define Initial State\n",
    "\n",
    "This is data that we should be retrieving from each meeting invite.\n",
    "\n",
    "**DO NOT** change this data until the CRM DB has been updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcc4c7df-7a99-4862-b4ff-e84ed8dfc48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state = {\n",
    "        'client_name': 'Adam Clay',\n",
    "        'client_email': 'adam.clay@compass.com',\n",
    "        'meeting_timestamp': '2024-03-26 11:00:00',\n",
    "        'meeting_description': 'Compass - Annual Credit Facility Review Meeting',\n",
    "        'report_length': 'long'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e71339a6-c098-47df-bb6d-acb6bcb1dc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============================\n",
      "Generating report with length: long\n",
      "===============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = compiled_graph.invoke(init_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5ae11b7-32be-4660-b462-69fb01173a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = result['final_summary']\n",
    "\n",
    "# === Add the macro snapshot\n",
    "macro_table = generate_macro_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7200286f-fc4e-4de0-bbc9-df9486c1cf9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Compass - Annual Credit Facility Review Meeting: Briefing Document**\n",
      "\n",
      "**1. Past Email Summary**\n",
      "\n",
      "The $25M revolving credit facility for Compass was established following initial contact at the Real Estate Finance conference.  Bankwell Financial provided an indicative term sheet, which Compass accepted after internal review.  The definitive loan agreement was finalized, and the facility was funded.  Compass has since made several drawdowns, reporting a positive experience with the process and reporting mechanisms.\n",
      "\n",
      "**2. Recent Email Topics**\n",
      "\n",
      "* Compass reported positive experiences with the credit facility, noting straightforward drawdowns and clear reporting.\n",
      "* Bankwell Financial confirmed the official closing and funding of the $25M facility.\n",
      "* Bankwell Financial reiterated its commitment to ongoing support and assistance for Compass.\n",
      "* Both parties expressed satisfaction with the partnership and the smooth closing process.\n",
      "\n",
      "**4. Financial Overview**\n",
      "\n",
      "Compass reported record first-quarter 2025 results, with a 28.7% year-over-year revenue increase to $1.4 billion.  This strong performance is fueled by significant acquisitions, including the completed acquisition of @properties and ongoing negotiations to acquire Warren Buffett's real estate brokerage unit.  However, Compass faces legal challenges, including lawsuits related to real estate commissions and marketing rules.  The broader economic outlook is mixed. While some indicators, such as durable goods orders, remain strong, concerns persist about a potential recession due to rising costs, weak consumer confidence, and the impact of tariffs.  This uncertainty is reflected in the mixed performance of other companies in related sectors, with some, such as CRH (building materials), reporting strong profits, while others, like ASML (semiconductor equipment), experience fluctuating valuations.  The real estate market, while showing strength in Compass's performance, faces potential headwinds from a broader economic downturn.\n"
     ]
    }
   ],
   "source": [
    "full_report = summary #+ \"\\n\\n### Macro Financial Snapshot\\n\\n\" + macro_table\n",
    "print(full_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ae1f39b-e207-475f-bc60-94c89708b38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Traceback\n",
    "\n",
    "client_industry_summary = result.get('client_industry_sources', [])\n",
    "macro_news_summary = result.get('macro_news_sources', [])\n",
    "\n",
    "# Function to format and print client_industry and macro_news summaries\n",
    "def print_summary(summary, summary_name):\n",
    "    print(f\"\\n{summary_name}:\\n{'='*len(summary_name)}\")\n",
    "    for article in summary:\n",
    "        title = article.get('title', 'No Title')\n",
    "        snippet = article.get('snippet', 'No Snippet')\n",
    "        date = article.get('date', 'No Date')\n",
    "        link = article.get('link', '')\n",
    "        \n",
    "        # Extract the source from the domain of the link\n",
    "        source = link.split(\"/\")[2] if link else 'No Source'\n",
    "        \n",
    "        # Format and print the information\n",
    "        print(f\"Title: {title}\")\n",
    "        print(f\"Snippet: {snippet}\")\n",
    "        print(f\"Date: {date}\")\n",
    "        print(f\"Source: {source}\")\n",
    "        print(f\"Link: {link}\")  # Displaying the full link\n",
    "        print(\"-\" * 40)  # Just a separator for readability\n",
    "\n",
    "# Function to format and print the holdings summary\n",
    "def print_holdings_summary(holdings_sources):\n",
    "    print(\"\\nClient Holdings Summary:\")\n",
    "    print(\"========================\")\n",
    "\n",
    "    for holding, articles in holdings_sources.items():\n",
    "        print(f\"\\n--- {holding} ---\")\n",
    "        if not articles:\n",
    "            print(\"No news found.\")\n",
    "            continue\n",
    "\n",
    "        for article in articles:\n",
    "            title = article.get('title', 'No Title')\n",
    "            snippet = article.get('snippet', 'No Snippet')\n",
    "            date = article.get('date', 'No Date')\n",
    "            link = article.get('link', '')\n",
    "            source = link.split(\"/\")[2] if link else 'No Source'\n",
    "\n",
    "            print(f\"Title: {title}\")\n",
    "            print(f\"Snippet: {snippet}\")\n",
    "            print(f\"Date: {date}\")\n",
    "            print(f\"Source: {source}\")\n",
    "            print(f\"Link: {link}\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "# Print the formatted summaries\n",
    "#print_summary(client_industry_summary, 'Client Industry Summary')\n",
    "#print_summary(macro_news_summary, 'Macro News Summary')\n",
    "#print_holdings_summary(result.get(\"client_holdings_sources\", {}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1faebb0-baea-4104-8945-5097ff593ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import wrap\n",
    "from currensee.core import get_model, settings\n",
    "\n",
    "def chunk_sources_with_metadata(sources: dict[str, list[dict]], max_length: int = 1000) -> dict[str, tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Chunk each source's snippet and retain the original link with each chunk.\n",
    "    Returns a dict like { 'Client Industry Summary [1.1]': (chunk_text, source_url) }\n",
    "    \"\"\"\n",
    "    chunked = {}\n",
    "    for category, entries in sources.items():\n",
    "        for i, entry in enumerate(entries):\n",
    "            snippet = entry.get(\"snippet\", \"\")\n",
    "            link = entry.get(\"link\", \"\")\n",
    "            title = entry.get(\"title\", \"\")\n",
    "\n",
    "            full_text = f\"{title}\\n{snippet}\".strip()\n",
    "            chunks = wrap(full_text, max_length, break_long_words=False, replace_whitespace=False)\n",
    "\n",
    "            for j, chunk in enumerate(chunks):\n",
    "                key = f\"{category} [{i+1}.{j+1}]\"\n",
    "                chunked[key] = (chunk.strip(), link)\n",
    "    return chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec688ed0-bf3c-44dd-b66e-9664b09796b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_with_urls(summary: str, chunked_sources: dict[str, tuple[str, str]]) -> str:\n",
    "    formatted_sources = \"\\n\\n\".join(\n",
    "        f\"{key} (Source: {url}):\\n{chunk}\" for key, (chunk, url) in chunked_sources.items()\n",
    "    )\n",
    "\n",
    "    return f\"\"\"\n",
    "You are a financial analyst assistant. You generated the following summary:\n",
    "\n",
    "--- Summary ---\n",
    "{summary}\n",
    "\n",
    "You used these source snippets (each with its original URL):\n",
    "\n",
    "--- Sources ---\n",
    "{formatted_sources}\n",
    "\n",
    "Please map each claim from the summary to the URLs that support it. Format:\n",
    "\n",
    "- Summary claim: \"...\"\n",
    "  → Source URL(s): [\"https://...\"]\n",
    "\n",
    "Use only the URLs in the provided sources. Don't invent URLs.\n",
    "\"\"\"\n",
    "\n",
    "def format_holdings_sources(raw_sources):\n",
    "    if not raw_sources:\n",
    "        return []\n",
    "\n",
    "    formatted = []\n",
    "    for ticker, articles in raw_sources.items():\n",
    "        for article in articles:\n",
    "            formatted.append({\n",
    "                \"title\": article.get(\"title\", ticker),\n",
    "                \"snippet\": article.get(\"snippet\", \"\"),\n",
    "                \"link\": article.get(\"link\", \"\")\n",
    "            })\n",
    "    return formatted\n",
    "\n",
    "\n",
    "# Step 1: Get and chunk sources properly\n",
    "sources = {\n",
    "    \"Client Industry Summary\": result.get(\"client_industry_sources\", []),\n",
    "    \"Holdings Summary\": format_holdings_sources(result.get(\"client_holdings_sources\", {})),\n",
    "    \"Macro Summary\": result.get(\"macro_news_sources\", [])\n",
    "}\n",
    "\n",
    "chunked_sources = chunk_sources_with_metadata(sources)\n",
    "\n",
    "# Step 2: Compose prompt and ask LLM\n",
    "prompt = build_prompt_with_urls(summary, chunked_sources)\n",
    "\n",
    "# Step 3: Invoke LLM\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model = get_model(settings.DEFAULT_MODEL)\n",
    "response = model.invoke([HumanMessage(content=prompt)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ced2d2fe-bd62-4760-9871-6e7b6adeb479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Step 3.5: Filter the output to remove claims with no supporting URLs\n",
    "def filter_empty_sources(response_text: str) -> str:\n",
    "    # Split the output into individual claim blocks\n",
    "    claim_blocks = re.split(r\"\\n(?=- Summary claim:)\", response_text.strip())\n",
    "\n",
    "    # Keep only those blocks that contain at least one URL\n",
    "    filtered_blocks = [\n",
    "        block for block in claim_blocks\n",
    "        if not re.search(r'→ Source URL\\(s\\):\\s*\\[\\s*\\]\\s*(\\*.*\\*)?', block)\n",
    "    ]\n",
    "\n",
    "    return \"\\n\\n\".join(filtered_blocks)\n",
    "\n",
    "def extract_claim_url_pairs(response_text: str) -> list[tuple[str, list[str]]]:\n",
    "    \"\"\"\n",
    "    Extracts a list of (claim, urls) from the LLM's response.\n",
    "    \"\"\"\n",
    "    claim_url_pairs = []\n",
    "    blocks = re.findall(r'- Summary claim:\\s*\"(.*?)\"\\s*→ Source URL\\(s\\):\\s*(\\[.*?\\])', response_text, re.DOTALL)\n",
    "    for claim, urls_str in blocks:\n",
    "        try:\n",
    "            urls = eval(urls_str, {\"__builtins__\": None}, {})\n",
    "            if isinstance(urls, list) and all(isinstance(u, str) for u in urls):\n",
    "                claim_url_pairs.append((claim.strip(), urls))\n",
    "        except Exception:\n",
    "            continue\n",
    "    return claim_url_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ccf1848-946a-4cfb-9919-9207416f9e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def insert_links_into_summary(summary: str, claim_url_pairs: list[tuple[str, list[str]]]) -> str:\n",
    "#     \"\"\"\n",
    "#     Inserts Markdown-style [Source] links after corresponding claims in the summary.\n",
    "#     \"\"\"\n",
    "#     updated_summary = summary\n",
    "\n",
    "#     for claim, urls in claim_url_pairs:\n",
    "#         if len(urls) == 1:\n",
    "#             link_text = f\" ([Source]({urls[0]}))\"\n",
    "#         else:\n",
    "#             link_text = \" (\" + \", \".join(\n",
    "#                 f\"[Source {i+1}]({url})\" for i, url in enumerate(urls)\n",
    "#             ) + \")\"\n",
    "        \n",
    "#         # Escape regex special characters in the claim text\n",
    "#         pattern = re.escape(claim)\n",
    "#         replacement = f'{claim}{link_text}'\n",
    "        \n",
    "#         updated_summary, count = re.subn(pattern, replacement, updated_summary, count=1)\n",
    "#         if count == 0:\n",
    "#             print(f\"⚠️ Could not find claim in summary: '{claim}'\")\n",
    "\n",
    "#     return updated_summary\n",
    "\n",
    "\n",
    "def insert_links_into_summary(summary: str, claim_url_pairs: list[tuple[str, list[str]]]) -> str:\n",
    "    \"\"\"\n",
    "    Inserts Markdown-style [Source] links after corresponding claims in the summary.\n",
    "    Only includes up to 3 sources per claim (truncates any extra).\n",
    "    \"\"\"\n",
    "    updated_summary = summary\n",
    "\n",
    "    for claim, urls in claim_url_pairs:\n",
    "        truncated_urls = urls[:3]  # ⛔ Truncate to at most 3 URLs\n",
    "\n",
    "        if len(truncated_urls) == 1:\n",
    "            link_text = f\" ([Source]({truncated_urls[0]}))\"\n",
    "        else:\n",
    "            link_text = \" (\" + \", \".join(\n",
    "                f\"[Source {i+1}]({url})\" for i, url in enumerate(truncated_urls)\n",
    "            ) + \")\"\n",
    "\n",
    "        pattern = re.escape(claim)\n",
    "        replacement = f'{claim}{link_text}'\n",
    "\n",
    "        updated_summary, count = re.subn(pattern, replacement, updated_summary, count=1)\n",
    "        if count == 0:\n",
    "            print(f\"⚠️ Could not find claim in summary: '{claim}'\")\n",
    "\n",
    "    return updated_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "755652f8-9691-440d-bf4e-62c95fc06688",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.invoke([HumanMessage(content=prompt)])\n",
    "filtered_output = filter_empty_sources(response.content)\n",
    "claim_url_pairs = extract_claim_url_pairs(filtered_output)\n",
    "linked_summary = insert_links_into_summary(summary, claim_url_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa5969e5-3294-4a60-aa49-75356207414c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 03:36:10,699 - fontTools.ttLib.ttFont - DEBUG - Reading 'maxp' table from disk\n",
      "2025-05-20 03:36:10,701 - fontTools.ttLib.ttFont - DEBUG - Decompiling 'maxp' table\n",
      "2025-05-20 03:36:10,702 - fontTools.subset.timer - DEBUG - Took 0.003s to load 'maxp'\n",
      "2025-05-20 03:36:10,702 - fontTools.subset.timer - DEBUG - Took 0.000s to prune 'maxp'\n",
      "2025-05-20 03:36:10,705 - fontTools.subset - INFO - maxp pruned\n",
      "2025-05-20 03:36:10,708 - fontTools.ttLib.ttFont - DEBUG - Reading 'cmap' table from disk\n",
      "2025-05-20 03:36:10,710 - fontTools.ttLib.ttFont - DEBUG - Decompiling 'cmap' table\n",
      "2025-05-20 03:36:10,713 - fontTools.ttLib.ttFont - DEBUG - Reading 'post' table from disk\n",
      "2025-05-20 03:36:10,717 - fontTools.ttLib.ttFont - DEBUG - Decompiling 'post' table\n",
      "2025-05-20 03:36:10,736 - fontTools.subset.timer - DEBUG - Took 0.028s to load 'cmap'\n",
      "2025-05-20 03:36:10,737 - fontTools.subset.timer - DEBUG - Took 0.000s to prune 'cmap'\n",
      "2025-05-20 03:36:10,739 - fontTools.subset - INFO - cmap pruned\n",
      "2025-05-20 03:36:10,740 - fontTools.subset - INFO - fpgm dropped\n",
      "2025-05-20 03:36:10,741 - fontTools.subset - INFO - prep dropped\n",
      "2025-05-20 03:36:10,741 - fontTools.subset - INFO - cvt  dropped\n",
      "2025-05-20 03:36:10,742 - fontTools.subset - INFO - kern dropped\n",
      "2025-05-20 03:36:10,744 - fontTools.subset.timer - DEBUG - Took 0.000s to load 'post'\n",
      "2025-05-20 03:36:10,745 - fontTools.subset.timer - DEBUG - Took 0.000s to prune 'post'\n",
      "2025-05-20 03:36:10,746 - fontTools.subset - INFO - post pruned\n",
      "2025-05-20 03:36:10,750 - fontTools.subset - INFO - GPOS dropped\n",
      "2025-05-20 03:36:10,751 - fontTools.subset - INFO - GSUB dropped\n",
      "2025-05-20 03:36:10,753 - fontTools.ttLib.ttFont - DEBUG - Reading 'glyf' table from disk\n",
      "2025-05-20 03:36:10,755 - fontTools.ttLib.ttFont - DEBUG - Decompiling 'glyf' table\n",
      "2025-05-20 03:36:10,756 - fontTools.ttLib.ttFont - DEBUG - Reading 'loca' table from disk\n",
      "2025-05-20 03:36:10,758 - fontTools.ttLib.ttFont - DEBUG - Decompiling 'loca' table\n",
      "2025-05-20 03:36:10,759 - fontTools.ttLib.ttFont - DEBUG - Reading 'head' table from disk\n",
      "2025-05-20 03:36:10,760 - fontTools.ttLib.ttFont - DEBUG - Decompiling 'head' table\n",
      "2025-05-20 03:36:10,993 - fontTools.subset.timer - DEBUG - Took 0.241s to load 'glyf'\n",
      "2025-05-20 03:36:10,995 - fontTools.subset.timer - DEBUG - Took 0.000s to prune 'glyf'\n",
      "2025-05-20 03:36:10,997 - fontTools.subset - INFO - glyf pruned\n",
      "2025-05-20 03:36:11,000 - fontTools.subset.timer - DEBUG - Took 0.002s to close glyph list over 'cmap'\n",
      "2025-05-20 03:36:11,001 - fontTools.subset - INFO - Added gid0 to subset\n",
      "2025-05-20 03:36:11,002 - fontTools.subset - INFO - Closing glyph list over 'MATH': 65 glyphs before\n",
      "2025-05-20 03:36:11,004 - fontTools.subset - INFO - Glyph names: ['.notdef', 'A', 'B', 'C', 'D', 'E', 'F', 'H', 'L', 'M', 'O', 'P', 'R', 'S', 'T', 'W', 'a', 'asterisk', 'at', 'b', 'c', 'colon', 'comma', 'd', 'dollar', 'e', 'eight', 'f', 'fi', 'five', 'fl', 'four', 'g', 'h', 'hyphen', 'i', 'k', 'l', 'm', 'n', 'o', 'one', 'p', 'parenleft', 'parenright', 'percent', 'period', 'q', 'quotesingle', 'r', 's', 'seven', 'space', 't', 'three', 'two', 'u', 'uniFB00', 'uniFB03', 'v', 'w', 'x', 'y', 'z', 'zero']\n",
      "2025-05-20 03:36:11,009 - fontTools.subset - INFO - Glyph IDs:   [0, 3, 7, 8, 10, 11, 12, 13, 15, 16, 17, 19, 20, 21, 22, 23, 24, 26, 27, 29, 35, 36, 37, 38, 39, 40, 41, 43, 47, 48, 50, 51, 53, 54, 55, 58, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 5041, 5042, 5043, 5044]\n",
      "2025-05-20 03:36:11,011 - fontTools.ttLib.ttFont - DEBUG - Reading 'MATH' table from disk\n",
      "2025-05-20 03:36:11,012 - fontTools.ttLib.ttFont - DEBUG - Decompiling 'MATH' table\n",
      "2025-05-20 03:36:11,018 - fontTools.subset - INFO - Closed glyph list over 'MATH': 71 glyphs after\n",
      "2025-05-20 03:36:11,020 - fontTools.subset - INFO - Glyph names: ['.notdef', 'A', 'B', 'C', 'D', 'E', 'F', 'H', 'L', 'M', 'O', 'P', 'R', 'S', 'T', 'W', 'a', 'asterisk', 'at', 'b', 'c', 'colon', 'comma', 'd', 'dollar', 'e', 'eight', 'f', 'fi', 'five', 'fl', 'four', 'g', 'h', 'hyphen', 'i', 'k', 'l', 'm', 'n', 'o', 'one', 'p', 'parenleft', 'parenright', 'percent', 'period', 'q', 'quotesingle', 'r', 's', 'seven', 'space', 't', 'three', 'two', 'u', 'uni239B', 'uni239C', 'uni239D', 'uni239E', 'uni239F', 'uni23A0', 'uniFB00', 'uniFB03', 'v', 'w', 'x', 'y', 'z', 'zero']\n",
      "2025-05-20 03:36:11,021 - fontTools.subset - INFO - Glyph IDs:   [0, 3, 7, 8, 10, 11, 12, 13, 15, 16, 17, 19, 20, 21, 22, 23, 24, 26, 27, 29, 35, 36, 37, 38, 39, 40, 41, 43, 47, 48, 50, 51, 53, 54, 55, 58, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 3507, 3508, 3509, 3510, 3511, 3512, 5041, 5042, 5043, 5044]\n",
      "2025-05-20 03:36:11,022 - fontTools.subset.timer - DEBUG - Took 0.020s to close glyph list over 'MATH'\n",
      "2025-05-20 03:36:11,024 - fontTools.subset - INFO - Closing glyph list over 'glyf': 71 glyphs before\n",
      "2025-05-20 03:36:11,025 - fontTools.subset - INFO - Glyph names: ['.notdef', 'A', 'B', 'C', 'D', 'E', 'F', 'H', 'L', 'M', 'O', 'P', 'R', 'S', 'T', 'W', 'a', 'asterisk', 'at', 'b', 'c', 'colon', 'comma', 'd', 'dollar', 'e', 'eight', 'f', 'fi', 'five', 'fl', 'four', 'g', 'h', 'hyphen', 'i', 'k', 'l', 'm', 'n', 'o', 'one', 'p', 'parenleft', 'parenright', 'percent', 'period', 'q', 'quotesingle', 'r', 's', 'seven', 'space', 't', 'three', 'two', 'u', 'uni239B', 'uni239C', 'uni239D', 'uni239E', 'uni239F', 'uni23A0', 'uniFB00', 'uniFB03', 'v', 'w', 'x', 'y', 'z', 'zero']\n",
      "2025-05-20 03:36:11,028 - fontTools.subset - INFO - Glyph IDs:   [0, 3, 7, 8, 10, 11, 12, 13, 15, 16, 17, 19, 20, 21, 22, 23, 24, 26, 27, 29, 35, 36, 37, 38, 39, 40, 41, 43, 47, 48, 50, 51, 53, 54, 55, 58, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 3507, 3508, 3509, 3510, 3511, 3512, 5041, 5042, 5043, 5044]\n",
      "2025-05-20 03:36:11,029 - fontTools.subset - INFO - Closed glyph list over 'glyf': 71 glyphs after\n",
      "2025-05-20 03:36:11,031 - fontTools.subset - INFO - Glyph names: ['.notdef', 'A', 'B', 'C', 'D', 'E', 'F', 'H', 'L', 'M', 'O', 'P', 'R', 'S', 'T', 'W', 'a', 'asterisk', 'at', 'b', 'c', 'colon', 'comma', 'd', 'dollar', 'e', 'eight', 'f', 'fi', 'five', 'fl', 'four', 'g', 'h', 'hyphen', 'i', 'k', 'l', 'm', 'n', 'o', 'one', 'p', 'parenleft', 'parenright', 'percent', 'period', 'q', 'quotesingle', 'r', 's', 'seven', 'space', 't', 'three', 'two', 'u', 'uni239B', 'uni239C', 'uni239D', 'uni239E', 'uni239F', 'uni23A0', 'uniFB00', 'uniFB03', 'v', 'w', 'x', 'y', 'z', 'zero']\n",
      "2025-05-20 03:36:11,033 - fontTools.subset - INFO - Glyph IDs:   [0, 3, 7, 8, 10, 11, 12, 13, 15, 16, 17, 19, 20, 21, 22, 23, 24, 26, 27, 29, 35, 36, 37, 38, 39, 40, 41, 43, 47, 48, 50, 51, 53, 54, 55, 58, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 3507, 3508, 3509, 3510, 3511, 3512, 5041, 5042, 5043, 5044]\n",
      "2025-05-20 03:36:11,034 - fontTools.subset.timer - DEBUG - Took 0.010s to close glyph list over 'glyf'\n",
      "2025-05-20 03:36:11,047 - fontTools.subset - INFO - Retaining 71 glyphs\n",
      "2025-05-20 03:36:11,051 - fontTools.subset - INFO - head subsetting not needed\n",
      "2025-05-20 03:36:11,052 - fontTools.subset - INFO - hhea subsetting not needed\n",
      "2025-05-20 03:36:11,053 - fontTools.subset - INFO - maxp subsetting not needed\n",
      "2025-05-20 03:36:11,055 - fontTools.subset - INFO - OS/2 subsetting not needed\n",
      "2025-05-20 03:36:11,057 - fontTools.ttLib.ttFont - DEBUG - Reading 'hmtx' table from disk\n",
      "2025-05-20 03:36:11,058 - fontTools.ttLib.ttFont - DEBUG - Decompiling 'hmtx' table\n",
      "2025-05-20 03:36:11,060 - fontTools.ttLib.ttFont - DEBUG - Reading 'hhea' table from disk\n",
      "2025-05-20 03:36:11,061 - fontTools.ttLib.ttFont - DEBUG - Decompiling 'hhea' table\n",
      "2025-05-20 03:36:11,074 - fontTools.subset.timer - DEBUG - Took 0.017s to subset 'hmtx'\n",
      "2025-05-20 03:36:11,075 - fontTools.subset - INFO - hmtx subsetted\n",
      "2025-05-20 03:36:11,080 - fontTools.subset.timer - DEBUG - Took 0.004s to subset 'cmap'\n",
      "2025-05-20 03:36:11,082 - fontTools.subset - INFO - cmap subsetted\n",
      "2025-05-20 03:36:11,083 - fontTools.subset - INFO - loca subsetting not needed\n",
      "2025-05-20 03:36:11,084 - fontTools.subset.timer - DEBUG - Took 0.000s to subset 'post'\n",
      "2025-05-20 03:36:11,085 - fontTools.subset - INFO - post subsetted\n",
      "2025-05-20 03:36:11,086 - fontTools.subset - INFO - gasp subsetting not needed\n",
      "2025-05-20 03:36:11,087 - fontTools.subset - INFO - FFTM NOT subset; don't know how to subset\n",
      "2025-05-20 03:36:11,088 - fontTools.subset.timer - DEBUG - Took 0.000s to subset 'MATH'\n",
      "2025-05-20 03:36:11,090 - fontTools.subset - INFO - MATH subsetted\n",
      "2025-05-20 03:36:11,091 - fontTools.ttLib.ttFont - DEBUG - Reading 'GDEF' table from disk\n",
      "2025-05-20 03:36:11,092 - fontTools.ttLib.ttFont - DEBUG - Decompiling 'GDEF' table\n",
      "2025-05-20 03:36:11,096 - fontTools.subset.timer - DEBUG - Took 0.006s to subset 'GDEF'\n",
      "2025-05-20 03:36:11,098 - fontTools.subset - INFO - GDEF subsetted\n",
      "2025-05-20 03:36:11,099 - fontTools.subset - INFO - name subsetting not needed\n",
      "2025-05-20 03:36:11,105 - fontTools.subset.timer - DEBUG - Took 0.005s to subset 'glyf'\n",
      "2025-05-20 03:36:11,106 - fontTools.subset - INFO - glyf subsetted\n",
      "2025-05-20 03:36:11,107 - fontTools.subset.timer - DEBUG - Took 0.000s to subset GlyphOrder\n",
      "2025-05-20 03:36:11,112 - fontTools.subset.timer - DEBUG - Took 0.000s to prune 'head'\n",
      "2025-05-20 03:36:11,114 - fontTools.subset - INFO - head pruned\n",
      "2025-05-20 03:36:11,117 - fontTools.ttLib.ttFont - DEBUG - Reading 'OS/2' table from disk\n",
      "2025-05-20 03:36:11,118 - fontTools.ttLib.ttFont - DEBUG - Decompiling 'OS/2' table\n",
      "2025-05-20 03:36:11,119 - fontTools.subset - INFO - OS/2 Unicode ranges pruned: [0, 62]\n",
      "2025-05-20 03:36:11,120 - fontTools.subset - INFO - OS/2 CodePage ranges pruned: [0]\n",
      "2025-05-20 03:36:11,123 - fontTools.subset.timer - DEBUG - Took 0.002s to prune 'glyf'\n",
      "2025-05-20 03:36:11,124 - fontTools.subset - INFO - glyf pruned\n",
      "2025-05-20 03:36:11,126 - fontTools.subset.timer - DEBUG - Took 0.000s to prune 'GDEF'\n",
      "2025-05-20 03:36:11,127 - fontTools.subset - INFO - GDEF pruned\n",
      "2025-05-20 03:36:11,127 - fontTools.ttLib.ttFont - DEBUG - Reading 'name' table from disk\n",
      "2025-05-20 03:36:11,128 - fontTools.ttLib.ttFont - DEBUG - Decompiling 'name' table\n",
      "2025-05-20 03:36:11,129 - fontTools.ttLib.ttFont - DEBUG - Reading 'gasp' table from disk\n",
      "2025-05-20 03:36:11,130 - fontTools.ttLib.ttFont - DEBUG - Decompiling 'gasp' table\n",
      "2025-05-20 03:36:11,131 - fontTools.ttLib.ttFont - DEBUG - Reading 'FFTM' table from disk\n",
      "2025-05-20 03:36:11,132 - fontTools.ttLib.ttFont - DEBUG - Decompiling 'FFTM' table\n",
      "2025-05-20 03:36:11,132 - fontTools.subset.timer - DEBUG - Took 0.005s to prune 'name'\n",
      "2025-05-20 03:36:11,133 - fontTools.subset - INFO - name pruned\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PDF created: final_summary.pdf\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from weasyprint import HTML\n",
    "\n",
    "def convert_markdown_links_to_html(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts markdown-style links like [Source 1](https://example.com)\n",
    "    into <a href=\"https://example.com\">Source 1</a>\n",
    "    \"\"\"\n",
    "    return re.sub(\n",
    "        r'\\[([^\\]]+)\\]\\((https?://[^\\)]+)\\)',\n",
    "        r'<a href=\"\\2\" target=\"_blank\" rel=\"noopener noreferrer\">\\1</a>',\n",
    "        text\n",
    "    )\n",
    "\n",
    "def wrap_html(content: str) -> str:\n",
    "    \"\"\"\n",
    "    Wraps converted content in full HTML with proper styling.\n",
    "    \"\"\"\n",
    "    html_body = content.replace('\\n', '<br>')\n",
    "    return f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <meta charset=\"utf-8\">\n",
    "        <style>\n",
    "            body {{\n",
    "                font-family: Arial, sans-serif;\n",
    "                font-size: 14px;\n",
    "                line-height: 1.6;\n",
    "                color: #000;\n",
    "                padding: 40px;\n",
    "            }}\n",
    "            a {{\n",
    "                color: #0645AD;\n",
    "                text-decoration: underline;\n",
    "            }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        {html_body}\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "def generate_pdf_from_summary(linked_summary: str, output_file: str = \"final_summary.pdf\"):\n",
    "    \"\"\"\n",
    "    Converts markdown-like [Source](url) links to HTML, wraps it, and writes a working PDF.\n",
    "    \"\"\"\n",
    "    html_links = convert_markdown_links_to_html(linked_summary)\n",
    "    full_html = wrap_html(html_links)\n",
    "    HTML(string=full_html, base_url=\".\").write_pdf(output_file)\n",
    "    print(f\"✅ PDF created: {output_file}\")\n",
    "\n",
    "generate_pdf_from_summary(linked_summary, \"final_summary.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42660f43-25a1-4372-9d18-42511e49601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def insert_links_into_summary_html(summary: str, claim_url_pairs: list[tuple[str, list[str]]]) -> str:\n",
    "#     \"\"\"\n",
    "#     Inserts HTML-style <a> links after claims and returns a fully HTML-formatted summary.\n",
    "#     \"\"\"\n",
    "#     updated_summary = summary\n",
    "\n",
    "#     for claim, urls in claim_url_pairs:\n",
    "#         if len(urls) == 1:\n",
    "#             link_text = f' (<a href=\"{urls[0]}\" target=\"_blank\" rel=\"noopener noreferrer\" title=\"\">Source</a>)'\n",
    "#         else:\n",
    "#             link_text = \" (\" + \", \".join(\n",
    "#                 f'<a href=\"{url}\" target=\"_blank\" rel=\"noopener noreferrer\" title=\"\">Source {i+1}</a>'\n",
    "#                 for i, url in enumerate(urls)\n",
    "#             ) + \")\"\n",
    "\n",
    "#         pattern = re.escape(claim)\n",
    "#         replacement = f'{claim}{link_text}'\n",
    "\n",
    "#         updated_summary, count = re.subn(pattern, replacement, updated_summary, count=1)\n",
    "#         if count == 0:\n",
    "#             print(f\"⚠️ Could not find claim in summary: '{claim}'\")\n",
    "\n",
    "#     return updated_summary\n",
    "    \n",
    "# def format_html_paragraphs(text: str) -> str:\n",
    "#     lines = text.strip().split(\"\\n\")\n",
    "#     html_lines = []\n",
    "#     for line in lines:\n",
    "#         if line.startswith(\"* \"):\n",
    "#             html_lines.append(f\"<li>{line[2:]}</li>\")\n",
    "#         elif line.strip().startswith(\"**\") and line.strip().endswith(\"**\"):\n",
    "#             html_lines.append(f\"<h3>{line.strip('* ')}</h3>\")\n",
    "#         elif line.strip() == \"\":\n",
    "#             html_lines.append(\"<br>\")\n",
    "#         else:\n",
    "#             html_lines.append(f\"<p>{line.strip()}</p>\")\n",
    "#     return \"\\n\".join(html_lines)\n",
    "# linked_summary = insert_links_into_summary_html(summary, claim_url_pairs)\n",
    "# final_html = format_html_paragraphs(linked_summary)\n",
    "\n",
    "# with open(\"annotated_summary.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(f\"<html><body>{final_html}</body></html>\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-so_currensee-so_currensee",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "so_currensee",
   "language": "python",
   "name": "conda-env-so_currensee-so_currensee"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
